from enum import Enum
from typing import Any, Optional
from uuid import UUID

from pydantic import BaseModel, Field


class Role(str, Enum):
    """Defines the role of the message author in a conversation."""

    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"


class FinishReason(str, Enum):
    """Reason why the generation stopped."""

    STOP = "stop"
    LENGTH = "length"
    CONTENT_FILTER = "content_filter"


class Message(BaseModel):
    """A message in a conversation between user and assistant."""

    # The role of the message author
    role: Role

    # The content of the message
    content: str

    # Optional name identifying the message author
    name: Optional[str] = None


class GenerationOptions(BaseModel):
    """Options that control the text generation process."""

    # The maximum number of tokens to generate
    max_tokens: Optional[int] = None

    # The number of tokens to sample from the top of the distribution
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0)

    # Nucleus sampling parameter
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0)

    # Sequences that will stop generation if encountered
    stop_sequences: Optional[list[str]] = None

    # Whether to return log probabilities of the generated tokens
    logprobs: Optional[bool] = None

    # The number of tokens to sample from the top of the distribution
    top_logprobs: Optional[int] = None

    # Extensions for the generation options
    extensions: Optional[dict[str, Any]] = Field(
        default=None, description="Container for specification extensions"
    )


class Usage(BaseModel):
    """Token usage information for the request and response."""

    # The number of tokens in the prompt
    prompt_tokens: int

    # The number of tokens in the generated text
    completion_tokens: int

    # The total number of tokens used in the request and response
    total_tokens: int


class LogProbs(BaseModel):
    """Log probabilities for generated tokens."""

    # Map of tokens to their log probabilities
    token_logprobs: dict[str, float] = Field(
        description="Map of tokens to their log probabilities"
    )


class CompletionResponse(BaseModel):
    """Response from a text completion request."""

    # Unique identifier for this completion
    id: UUID

    # Name of the model used for generation
    model: str

    # The generated text
    text: str

    # Reason why the generation stopped
    finish_reason: Optional[FinishReason] = None

    # Token usage information
    usage: Usage

    # Provider-specific information
    provider_info: Optional[dict[str, Any]] = None

    # Log probabilities for generated tokens
    logprobs: Optional[LogProbs] = None


class ChatResponse(BaseModel):
    """Response from a chat completion request."""

    # The ID of the response (using UUID instead of string)
    id: UUID

    # Name of the model used for generation
    model: str

    # The message generated by the assistant
    message: Message

    # Reason why the generation stopped
    finish_reason: Optional[FinishReason] = None

    # Token usage information
    usage: Usage

    # Provider-specific information
    provider_info: Optional[dict[str, Any]] = None

    # Log probabilities for generated tokens
    logprobs: Optional[LogProbs] = None


# Method parameter models
class GenerateCompletionParams(BaseModel):
    """Parameters for text completion generation."""

    # The model identifier to use for generation
    model: str

    # The input text to generate from
    prompt: str

    # Additional parameters for the generation
    options: Optional[GenerationOptions] = None


class GenerateChatParams(BaseModel):
    """Parameters for chat completion generation."""

    # The model identifier to use for chat
    model: str

    # Array of message objects representing the conversation
    messages: list[Message]

    # Additional parameters for the generation
    options: Optional[GenerationOptions] = None
